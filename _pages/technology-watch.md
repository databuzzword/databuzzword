---
layout: page
title: Technology Watch
permalink: /watch/
---

# Deep Learning

## Nothing to Image
### Generative
#### Face
##### Disentangled Image Generation Through Structured Noise Injection
![https://github.com/yalharbi/StructuredNoiseInjection/blob/master/example_fakes_mouth.png](https://github.com/yalharbi/StructuredNoiseInjection/blob/master/example_fakes_mouth.png)
- [Code](https://github.com/yalharbi/StructuredNoiseInjection)
- [Paper](https://arxiv.org/abs/2004.12411)

```
@misc{alharbi2020disentangled,
    title={Disentangled Image Generation Through Structured Noise Injection},
    author={Yazeed Alharbi and Peter Wonka},
    year={2020},
    eprint={2004.12411},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

<iframe width="100%" src="https://www.youtube.com/embed/7h-7wso9E0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Image to Anything

### Image to Image
#### Super resolution
##### PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
![http://pulse.cs.duke.edu/assets/094.jpeg](http://pulse.cs.duke.edu/assets/094.jpeg)
- [Project Page](PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models)
- [Colab](https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true)
- [Code](https://github.com/adamian98/pulse)
- [Paper](https://arxiv.org/abs/2003.03808)

```
@InProceedings{PULSE_CVPR_2020, 
author = {Menon, Sachit and Damian, Alex and Hu, McCourt and Ravi, Nikhil and Rudin, Cynthia}, 
title = {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models}, 
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
month = {June}, 
year = {2020} 
}
```

##### Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining
![https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png](https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png)
- [Code](https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention)
- [Paper](https://arxiv.org/abs/2006.01424)

```
@inproceedings{Mei2020image,
  title={Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining},
  author={Mei, Yiqun and Fan, Yuchen and Zhou, Yuqian and Huang, Lichao and Huang, Thomas S and Shi, Humphrey},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}
```

#### Image-to-Image translation
##### DeepFaceDrawing: Deep Generation of Face Images from Sketches
![http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg](http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg)
- [Paper](http://geometrylearning.com/DeepFaceDrawing/)

<iframe width="100%" src="https://www.youtube.com/embed/HSunooUTwKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

##### UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)
![https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true](https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true)
- [Code](https://github.com/taki0112/UGATIT#paper--official-pytorch-code)
- [Paper](https://arxiv.org/abs/1907.10830)

```
@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
```

- [Author's Site](https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107€)

##### Selfie to Anime
![https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg](https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg?raw=true)
- [Project Page](https://selfie2anime.com/)
- [Code](https://github.com/t04glovern/selfie2anime)
- [API](https://market-place.ai.ovh.net/#!/apis/59a0426c-c148-4cff-a042-6cc148fcffa5/pages/06641de1-1b1c-4bd2-a41d-e11b1c3bd230)
- [Paper](https://github.com/t04glovern/selfie2anime/blob/master/assets/Deploying-Models-to-the-Masses.pdf)

```
@misc{kim2019ugatit,
    title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
    author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwanghee Lee},
    year={2019},
    eprint={1907.10830},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```
- [Author's Site](https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107)

#### Segmentation
##### Attention-Guided Hierarchical Structure Aggregation for Image Matting
![https://wukaoliu.github.io/HAttMatting/figures/visualization.png](https://wukaoliu.github.io/HAttMatting/figures/visualization.png)
- [Project Page](https://wukaoliu.github.io/HAttMatting/)
- [Code](https://github.com/wukaoliu/CVPR2020-HAttMatting)
- [Paper](https://wukaoliu.github.io/HAttMatting/)

```
@InProceedings{Qiao_2020_CVPR,
    author = {Qiao, Yu and Liu, Yuhao and Yang, Xin and Zhou, Dongsheng and Xu, Mingliang and Zhang, Qiang and Wei, Xiaopeng},
    title = {Attention-Guided Hierarchical Structure Aggregation for Image Matting},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
```

##### Foreground-aware Semantic Representations for Image Harmonization
![https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg](https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg)
- [Code](https://github.com/saic-vul/image_harmonization)
- [Paper](https://arxiv.org/abs/2006.00809)

```
@article{sofiiuk2020harmonization,
  title={Foreground-aware Semantic Representations for Image Harmonization},
  author={Konstantin Sofiiuk, Polina Popenova, Anton Konushin},
  journal={arXiv preprint arXiv:2006.00809},
  year={2020}
}
```

### Image to Text
##### Image Captioning
![https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg](https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg)
- [Code](https://github.com/jayeshsaita/image_captioning_pytorch)


### Image to Sound/Speech

### Image to Video


## Text to Anything

### Text to Image

### Text to Text
#### Code to Code
##### Unsupervised Translation of Programming Languages
- [Paper](https://arxiv.org/abs/2006.03511)

```
@misc{lachaux2020unsupervised,
    title={Unsupervised Translation of Programming Languages},
    author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
    year={2020},
    eprint={2006.03511},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<iframe width="100%" src="https://www.youtube.com/embed/xTzFJIknh7E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
### Text to Sound/Speech

### Text to Video


## Sound/Speech to Anything

### Sound/Speech to Image
##### Audio to Image Conversion
- [Code](https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda)

### Sound/Speech to Text
##### Speech Command Reognitiob
- [Code](https://github.com/jayeshsaita/Speech-Commands-Recognition)
- [Paper](https://www.linkedin.com/in/jayeshsaita)

### Sound/Speech to Sound/Speech

### Sound/Speech to Video



## Video to Anything

### Video to Video
#### Segmentation
##### MSeg : A Composite Dataset for Multi-domain Semantic Segmentation
![https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif](https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif)
- [Code](https://github.com/mseg-dataset)
- [Paper](https://vladlen.info/papers/MSeg.pdf)

```
@InProceedings{MSeg_2020_CVPR,
author = {Lambert, John and Zhuang, Liu and Sener, Ozan and Hays, James and Koltun, Vladlen},
title = {MSeg A Composite Dataset for Multi-domain Semantic Segmentation},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
year = {2020}
}
```

<iframe width="100%" src="https://www.youtube.com/embed/PzBK6K5gyyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

##### Motion Supervised co-part Segmentation
![https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true](https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true)
- [Code](https://github.com/AliaksandrSiarohin/motion-cosegmentation)
- [Paper](http://arxiv.org/abs/2004.03234)

```
@article{Siarohin_2020_motion,
  title={Motion Supervised co-part Segmentation},
  author={Siarohin, Aliaksandr and Roy, Subhankar and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={arXiv preprint},
  year={2020}
}
```

<iframe width="100%" src="https://www.youtube.com/embed/RJ4Nj1wV5iA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Video to Image

### Video to Text

### Video toSound/Speech

### Video to Video


# Inference
## Fastai
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce <a href="https://twitter.com/hashtag/fastinference?src=hash&amp;ref_src=twsrc%5Etfw">#fastinference</a>! The bad news: fastshap and ClassConfusion are gone. The good news: they moved to a new home! In this module we have all of the above plus some speed-up and QOL integrations into <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>&#39;s inference methods, see here: 1/<a href="https://t.co/SLgJahtSr5">https://t.co/SLgJahtSr5</a> <a href="https://t.co/1oFkMe4SsP">pic.twitter.com/1oFkMe4SsP</a></p>&mdash; Zach #masks4all Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1269818072577331200?ref_src=twsrc%5Etfw">June 8, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

