<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/databuzzword/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/databuzzword/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/databuzzword/">DataBuzzWord Blog!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/databuzzword/about/">About The Podcast</a><a class="page-link" href="/databuzzword/experimenta/">DataBuzzWord Experiments</a><a class="page-link" href="/databuzzword/reading-list/">Reading List</a><a class="page-link" href="/databuzzword/search/">Search</a><a class="page-link" href="/databuzzword/categories/">Tags</a><a class="page-link" href="/databuzzword/watch/">Technology Watch</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Technology Watch</h1>
  </header>

  <div class="post-content">
    <h1 id="machinedeep-learning-tools">Machine/Deep Learning tools</h1>
<h2 id="technics">Technics</h2>
<h3 id="text-summarization">Text summarization</h3>
<p>Great tutorial serie here</p>
<ul>
  <li><a href="https://medium.com/hackernoon/text-summarizer-using-deep-learning-made-easy-490880df6cd">Part 1</a></li>
  <li><a href="https://medium.com/hackernoon/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46">Part 2</a></li>
  <li><a href="https://medium.com/hackernoon/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0">Part 3</a></li>
  <li><a href="https://medium.com/hackernoon/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">Part 4</a></li>
  <li><a href="https://medium.com/hackernoon/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">Part 5</a></li>
  <li><a href="https://medium.com/hackernoon/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55">Part 6</a></li>
  <li><a href="https://medium.com/hackernoon/combination-of-abstractive-extractive-methods-for-text-summarization-tutorial-7-8a4fb85d67e2">Part 7</a></li>
  <li><a href="https://medium.com/hackernoon/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754">Part 8</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 9</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 10</a></li>
  <li><a href="https://github.com/theamrzaki/text_summurization_abstractive_methods">Code</a></li>
</ul>

<h3 id="transformers">Transformers</h3>
<ul>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The illustrated Transformers</a></li>
  <li><a href="https://arxiv.org/abs/2001.04451">Transformers explained</a></li>
  <li><a href="https://arxiv.org/abs/1706.03762">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<ul>
  <li><a href="https://app.wandb.ai/cayush/bert-finetuning/reports/Sentence-classification-with-Huggingface-BERT-and-W%26B--Vmlldzo4MDMwNA">Blog Text Classification using Transformers</a></li>
</ul>

<h3 id="bert">BERT</h3>
<h3 id="bart">BART</h3>
<h3 id="xlnet">XLNet</h3>
<h3 id="gpt">GPT</h3>
<h3 id="gpt2">GPT2</h3>
<h3 id="gpt3">GPT3</h3>

<h3 id="reformers">Reformers</h3>
<p><a href="https://arxiv.org/abs/2001.04451">Colab</a></p>

<h2 id="useful-libs">Useful Libs</h2>
<h3 id="wrapper">Wrapper</h3>
<h4 id="vision">Vision</h4>
<h4 id="text">Text</h4>
<h5 id="fastai-code-first-intro-to-natural-language-processing">fast.ai Code-First Intro to Natural Language Processing</h5>
<ul>
  <li><a href="https://www.fast.ai/2019/07/08/fastai-nlp/">Project Page</a></li>
  <li><a href="https://github.com/fastai/course-nlp">Code</a></li>
</ul>

<p>here is the associated tutorial serie:</p>
<iframe width="100%" src="https://www.youtube.com/embed/videoseries?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="text-1">Text</h3>
<h4 id="nltk">NLTK</h4>
<ul>
  <li><a href="https://www.nltk.org">Project Page</a></li>
  <li><a href="https://github.com/nltk/nltk">Code</a></li>
  <li><a href="https://www.nltk.org/index.html">Doc</a></li>
  <li><a href="https://cheatography.com/murenei/cheat-sheets/natural-language-processing-with-python-and-nltk/">Cheat Sheet</a></li>
</ul>

<p>I consider <a href="https://twitter.com/Sentdex">@SentDex</a> founder <a href="https://pythonprogramming.net">pythonprogramming.net</a> and <a href="https://www.youtube.com/channel/sentdex">https://www.youtube.com/channel/sentdex</a> as the best tutorial for NLTK</p>

<iframe width="100%" src="https://www.youtube.com/embed/videoseries?list=PLI142kNg_e0Q57BmOF9H4UnXiWNSVZZ-O" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="spacy">SpaCy</h4>
<ul>
  <li><a href="https://spacy.io/">Project Page</a></li>
  <li><a href="https://github.com/explosion/spaCy">Code</a></li>
  <li><a href="https://spacy.io/api/doc">Doc</a></li>
  <li><a href="https://www.datacamp.com/community/blog/spacy-cheatsheet">Cheat Sheet</a></li>
  <li><a href="https://twitter.com/spacy_io">Spacy on Twitter @spacy_io</a></li>
  <li><a href="https://www.linkedin.com/company/explosion-ai/">Spacy on Linkedin</a></li>
  <li><a href="https://www.youtube.com/c/ExplosionAI">Spacy on Youtube</a></li>
</ul>

<p>A good Spacy tutorial Youtube serie here :</p>

<iframe width="100%" src="https://www.youtube.com/embed/videoseries?list=PLJ39kWiJXSiz1LK8d_fyxb7FTn4mBYOsD" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Spacy channel :</p>
<iframe width="100%" src="https://www.youtube.com/embed/videoseries?list=PLBmcuObd5An559HbDr_alBnwVsGq-7uTF" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="transformers-huggingface">Transformers (HuggingFace)</h4>
<ul>
  <li><a href="https://huggingface.co/">Project Page</a></li>
  <li><a href="https://github.com/huggingface/transformers">Code</a></li>
  <li><a href="https://huggingface.co/transformers/">Doc</a></li>
  <li><a href="https://twitter.com/huggingface">HuggingFace on Twitter @HuggingFace</a></li>
  <li><a href="https://www.linkedin.com/company/huggingface/">HuggingFace on Linkedin</a></li>
</ul>

<h4 id="simple-transformers-based-on-huggingface">Simple Transformers (based on HuggingFace)</h4>
<ul>
  <li><a href="https://simpletransformers.ai">Project Page</a></li>
  <li><a href="https://github.com/ThilinaRajapakse/simpletransformers">Code</a></li>
  <li><a href="https://simpletransformers.ai/docs/installation/">Doc</a></li>
  <li><a href="https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3">Blog Post</a></li>
</ul>

<p>Simple Transformers is a wrapper on top of HuggingFace’s Transformer Library take makes it easy to setup and use, here is an example of binary classification :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">simpletransformers.classification</span> <span class="kn">import</span> <span class="n">ClassificationModel</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">transformers_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s">"transformers"</span><span class="p">)</span>
<span class="n">transformers_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

<span class="c1"># Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.
</span><span class="n">train_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example eval sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example eval sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>

<span class="c1"># Create a ClassificationModel
</span><span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationModel</span><span class="p">(</span><span class="s">'roberta'</span><span class="p">,</span> <span class="s">'roberta-base'</span><span class="p">)</span> <span class="c1"># You can set class weights by using the optional weight argument
</span>
<span class="c1"># Train the model
</span><span class="n">model</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>

<span class="c1"># Evaluate the model
</span><span class="n">result</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">,</span> <span class="n">wrong_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval_model</span><span class="p">(</span><span class="n">eval_df</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="hands-on">Hands-on</h2>
<h3 id="nlp">NLP</h3>
<ul>
  <li><a href="https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb">Modern NLP in Python</a></li>
</ul>

<p>| Tool                                             | Binary Classification | Multi-Label Classification | Question Answering | Tokenization | Generation | Named Entity Recognition |
|-|-|-|-|-|-|-|</p>

<h4 id="structured-data">Structured Data</h4>
<h3 id="automl">AutoML</h3>
<h4 id="autokeras">AutoKeras</h4>

<p><a href="https://autokeras.com">AutoKeras</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>

<span class="c1"># Prepare the dataset.
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>  <span class="c1"># array([7, 2, 1], dtype=uint8)
</span>
<span class="c1"># Initialize the ImageClassifier.
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="o">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Search for the best model.
</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># Evaluate on the testing data.
</span><span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {accuracy}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">accuracy</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{jin2019auto,
  title={Auto-Keras: An Efficient Neural Architecture Search System},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
  pages={1946--1956},
  year={2019},
  organization={ACM}
}
</code></pre></div></div>

<h4 id="ovhcloud-automl">OVHcloud autoML</h4>
<p><img src="https://labs.ovh.com/sites/default/files/inline-images/upload_source_1.png" alt="Demo" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/optimization_result_2.png" alt="Demo2" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/model_kpi_2.png" alt="Demo3" /></p>

<ul>
  <li><a href="https://labs.ovh.com/machine-learning-platform">Site</a></li>
  <li><a href="https://github.com/ovh/prescience-client">Code</a></li>
  <li><a href="https://gitter.im/ovh/ai">Forum</a></li>
</ul>

<h1 id="deep-learning-use-cases">Deep Learning use cases</h1>

<h2 id="nothing-to-image">Nothing to Image</h2>
<h3 id="generative">Generative</h3>
<h4 id="face">Face</h4>
<h5 id="disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</h5>
<p><img src="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" alt="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" /></p>
<ul>
  <li><a href="https://github.com/yalharbi/StructuredNoiseInjection">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.12411">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{alharbi2020disentangled,
    title={Disentangled Image Generation Through Structured Noise Injection},
    author={Yazeed Alharbi and Peter Wonka},
    year={2020},
    eprint={2004.12411},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/7h-7wso9E0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="image-to-anything">Image to Anything</h2>

<h3 id="image-to-image">Image to Image</h3>
<h4 id="inpainting">Inpainting</h4>
<h5 id="high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</h5>
<p><img src="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" alt="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" /></p>
<ul>
  <li><a href="https://zengxianyu.github.io/iic/">Project Page</a></li>
  <li><a href="http://47.57.135.203:2333/">APP</a></li>
  <li><a href="https://arxiv.org/abs/2005.11742">Paper</a></li>
</ul>

<h5 id="edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</h5>
<p><img src="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" alt="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/edge-connect">Code</a></li>
  <li><a href="https://arxiv.org/abs/1901.00212">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  journal={arXiv preprint},
  year={2019},
}
</code></pre></div></div>

<h5 id="progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</h5>
<p><img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_damaged2.png?raw=true" alt="Before" />
<img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_final2.png?raw=true" alt="After" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/Inpainting_FRRN">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10478">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{guo2019progressive,
    title={Progressive Image Inpainting with Full-Resolution Residual Network},
    author={Zongyu Guo and Zhibo Chen and Tao Yu and Jiale Chen and Sen Liu},
    year={2019},
    eprint={1907.10478},
    archivePrefix={arXiv},
    primaryClass={eess.IV}
}
</code></pre></div></div>

<h4 id="super-resolution">Super resolution</h4>
<h5 id="pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</h5>
<p><img src="http://pulse.cs.duke.edu/assets/094.jpeg" alt="http://pulse.cs.duke.edu/assets/094.jpeg" /></p>
<ul>
  <li><a href="PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models">Project Page</a></li>
  <li><a href="https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true">Colab</a></li>
  <li><a href="https://github.com/adamian98/pulse">Code</a></li>
  <li><a href="https://arxiv.org/abs/2003.03808">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{PULSE_CVPR_2020, 
author = {Menon, Sachit and Damian, Alex and Hu, McCourt and Ravi, Nikhil and Rudin, Cynthia}, 
title = {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models}, 
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
month = {June}, 
year = {2020} 
}
</code></pre></div></div>

<h5 id="image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</h5>
<p><img src="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" alt="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" /></p>
<ul>
  <li><a href="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.01424">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Mei2020image,
  title={Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining},
  author={Mei, Yiqun and Fan, Yuchen and Zhou, Yuqian and Huang, Lichao and Huang, Thomas S and Shi, Humphrey},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}
</code></pre></div></div>

<h4 id="image-to-image-translation">Image-to-Image translation</h4>
<h5 id="deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</h5>
<p><img src="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" alt="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" /></p>
<ul>
  <li><a href="http://geometrylearning.com/DeepFaceDrawing/">Paper</a></li>
</ul>

<iframe width="100%" src="https://www.youtube.com/embed/HSunooUTwKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</h5>
<p><img src="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" alt="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/taki0112/UGATIT#paper--official-pytorch-code">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10830">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
</code></pre></div></div>

<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107€">Author’s Site</a></li>
</ul>

<h5 id="selfie-to-anime">Selfie to Anime</h5>
<p><img src="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg?raw=true" alt="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg" /></p>
<ul>
  <li><a href="https://selfie2anime.com/">Project Page</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime">Code</a></li>
  <li><a href="https://market-place.ai.ovh.net/#!/apis/59a0426c-c148-4cff-a042-6cc148fcffa5/pages/06641de1-1b1c-4bd2-a41d-e11b1c3bd230">API</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime/blob/master/assets/Deploying-Models-to-the-Masses.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kim2019ugatit,
    title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
    author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwanghee Lee},
    year={2019},
    eprint={1907.10830},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>
<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107">Author’s Site</a></li>
</ul>

<h4 id="segmentation">Segmentation</h4>
<h5 id="poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</h5>
<p><img src="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" alt="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" /></p>
<ul>
  <li><a href="https://gitlab.com/irafm-ai/poly-yolo">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.13243">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{hurtik2020polyyolo,
    title={Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3},
    author={Petr Hurtik and Vojtech Molek and Jan Hula and Marek Vajgl and Pavel Vlasanek and Tomas Nejezchleba},
    year={2020},
    eprint={2005.13243},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/2KxNnEV-Zes" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</h5>
<p><img src="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" alt="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" /></p>
<ul>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Project Page</a></li>
  <li><a href="https://github.com/wukaoliu/CVPR2020-HAttMatting">Code</a></li>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Qiao_2020_CVPR,
    author = {Qiao, Yu and Liu, Yuhao and Yang, Xin and Zhou, Dongsheng and Xu, Mingliang and Zhang, Qiang and Wei, Xiaopeng},
    title = {Attention-Guided Hierarchical Structure Aggregation for Image Matting},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</code></pre></div></div>

<h5 id="foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</h5>
<p><img src="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" alt="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" /></p>
<ul>
  <li><a href="https://github.com/saic-vul/image_harmonization">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.00809">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sofiiuk2020harmonization,
  title={Foreground-aware Semantic Representations for Image Harmonization},
  author={Konstantin Sofiiuk, Polina Popenova, Anton Konushin},
  journal={arXiv preprint arXiv:2006.00809},
  year={2020}
}
</code></pre></div></div>

<h3 id="image-to-text">Image to Text</h3>
<h5 id="image-captioning">Image Captioning</h5>
<p><img src="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" alt="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" /></p>
<ul>
  <li><a href="https://github.com/jayeshsaita/image_captioning_pytorch">Code</a></li>
</ul>

<h3 id="image-to-soundspeech">Image to Sound/Speech</h3>

<h3 id="image-to-video">Image to Video</h3>

<h2 id="text-to-anything">Text to Anything</h2>

<h3 id="text-to-image">Text to Image</h3>

<h3 id="text-to-text">Text to Text</h3>
<h4 id="text-generation">Text Generation</h4>
<h5 id="next-word-prediction">Next Word Prediction</h5>
<p><img src="https://raw.githubusercontent.com/renatoviolin/next_word_prediction/master/word_prediction.gif" alt="UI" /></p>
<ul>
  <li><a href="https://github.com/renatoviolin/next_word_prediction">Code</a></li>
</ul>

<h4 id="code-to-code">Code to Code</h4>
<h5 id="unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</h5>
<ul>
  <li><a href="https://arxiv.org/abs/2006.03511">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{lachaux2020unsupervised,
    title={Unsupervised Translation of Programming Languages},
    author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
    year={2020},
    eprint={2006.03511},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/xTzFJIknh7E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<h3 id="text-to-soundspeech">Text to Sound/Speech</h3>
<h4 id="pitchtron-towards-audiobook-generation-from-ordinary-peoples-voices">Pitchtron: Towards audiobook generation from ordinary people’s voices</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2005.10456">Paper</a></li>
  <li><a href="https://github.com/hash2430/pitchtron">Code</a></li>
</ul>

<h3 id="text-to-video">Text to Video</h3>

<h2 id="soundspeech-to-anything">Sound/Speech to Anything</h2>

<h3 id="soundspeech-to-image">Sound/Speech to Image</h3>
<h5 id="audio-to-image-conversion">Audio to Image Conversion</h5>
<ul>
  <li><a href="https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda">Code</a></li>
</ul>

<h3 id="soundspeech-to-text">Sound/Speech to Text</h3>
<h5 id="speech-command-reognition">Speech Command Reognition</h5>
<ul>
  <li><a href="https://github.com/jayeshsaita/Speech-Commands-Recognition">Code</a></li>
  <li><a href="https://www.linkedin.com/in/jayeshsaita">Paper</a></li>
</ul>

<h3 id="soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</h3>

<h3 id="soundspeech-to-video">Sound/Speech to Video</h3>

<h2 id="video-to-anything">Video to Anything</h2>

<h3 id="video-to-video">Video to Video</h3>
<h4 id="segmentation-1">Segmentation</h4>
<h5 id="mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</h5>
<p><img src="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" alt="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" /></p>
<ul>
  <li><a href="https://github.com/mseg-dataset">Code</a></li>
  <li><a href="https://vladlen.info/papers/MSeg.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{MSeg_2020_CVPR,
author = {Lambert, John and Zhuang, Liu and Sener, Ozan and Hays, James and Koltun, Vladlen},
title = {MSeg A Composite Dataset for Multi-domain Semantic Segmentation},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
year = {2020}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/PzBK6K5gyyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</h5>
<p><img src="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" alt="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/AliaksandrSiarohin/motion-cosegmentation">Code</a></li>
  <li><a href="http://arxiv.org/abs/2004.03234">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Siarohin_2020_motion,
  title={Motion Supervised co-part Segmentation},
  author={Siarohin, Aliaksandr and Roy, Subhankar and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={arXiv preprint},
  year={2020}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/RJ4Nj1wV5iA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="video-to-image">Video to Image</h3>

<h3 id="video-to-text">Video to Text</h3>

<h3 id="video-tosoundspeech">Video toSound/Speech</h3>

<h3 id="video-to-video-1">Video to Video</h3>

<h1 id="inference">Inference</h1>
<h2 id="fastai">Fastai</h2>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce <a href="https://twitter.com/hashtag/fastinference?src=hash&amp;ref_src=twsrc%5Etfw">#fastinference</a>! The bad news: fastshap and ClassConfusion are gone. The good news: they moved to a new home! In this module we have all of the above plus some speed-up and QOL integrations into <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>&#39;s inference methods, see here: 1/<a href="https://t.co/SLgJahtSr5">https://t.co/SLgJahtSr5</a> <a href="https://t.co/1oFkMe4SsP">pic.twitter.com/1oFkMe4SsP</a></p>&mdash; Zach #masks4all Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1269818072577331200?ref_src=twsrc%5Etfw">June 8, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://jianjye.com/p/deploy-fastai-digitalocean-ubuntu-flask-supervisor/">How to deploy Fastai on Ubuntu</a></p>
<h2 id="huggingface">HuggingFace</h2>
<p><a href="https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18">Deploying HuggingFace to Production</a></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/databuzzword/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/databuzzword/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/databuzzword/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jqueguiner" title="jqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://stackoverflow.com/users/users%2F786021%2Fguignol" title="users/786021/guignol"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#stackoverflow"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/jlqueguiner" title="jlqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jilijeanlouis" title="jilijeanlouis"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://www.youtube.com/UCwPhv5c2FQf6Ai3E6rT8_cw" title="UCwPhv5c2FQf6Ai3E6rT8_cw"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#youtube"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
