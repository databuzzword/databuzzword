<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/databuzzword/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/databuzzword/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/databuzzword/">DataBuzzWord Blog!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/databuzzword/about/">About The Podcast</a><a class="page-link" href="/databuzzword/experimenta/">DataBuzzWord Experiments</a><a class="page-link" href="/databuzzword/reading-list/">Reading List</a><a class="page-link" href="/databuzzword/search/">Search</a><a class="page-link" href="/databuzzword/categories/">Tags</a><a class="page-link" href="/databuzzword/watch/">Technology Watch</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Technology Watch</h1>
  </header>

  <div class="post-content">
    <h1 id="machinedeep-learning-tools">Machine/Deep Learning tools</h1>
<h2 id="common-issues">Common issues</h2>
<h3 id="unbalanced-dataset">Unbalanced Dataset</h3>
<h4 id="solution--oversampling">Solution : Oversampling</h4>
<ul>
  <li><a href="https://www.kaggle.com/tanlikesmath/oversampling-mnist-with-fastai">Oversampling with FastAI</a></li>
</ul>

<h2 id="technics">Technics</h2>
<h3 id="text-summarization">Text summarization</h3>
<p>Great tutorial serie here</p>
<ul>
  <li><a href="https://medium.com/hackernoon/text-summarizer-using-deep-learning-made-easy-490880df6cd">Part 1</a></li>
  <li><a href="https://medium.com/hackernoon/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46">Part 2</a></li>
  <li><a href="https://medium.com/hackernoon/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0">Part 3</a></li>
  <li><a href="https://medium.com/hackernoon/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">Part 4</a></li>
  <li><a href="https://medium.com/hackernoon/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">Part 5</a></li>
  <li><a href="https://medium.com/hackernoon/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55">Part 6</a></li>
  <li><a href="https://medium.com/hackernoon/combination-of-abstractive-extractive-methods-for-text-summarization-tutorial-7-8a4fb85d67e2">Part 7</a></li>
  <li><a href="https://medium.com/hackernoon/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754">Part 8</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 9</a></li>
  <li><a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c">Part 10</a></li>
  <li><a href="https://github.com/theamrzaki/text_summurization_abstractive_methods">Code</a></li>
</ul>

<h3 id="transformers">Transformers</h3>
<ul>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The illustrated Transformers</a></li>
  <li><a href="https://arxiv.org/abs/2001.04451">Transformers explained</a></li>
  <li><a href="https://arxiv.org/abs/1706.03762">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<h3 id="bert">BERT</h3>
<h4 id="blogs">Blogs</h4>
<ul>
  <li><a href="https://app.wandb.ai/cayush/bert-finetuning/reports/Sentence-classification-with-Huggingface-BERT-and-W%26B--Vmlldzo4MDMwNA">Blog Text Classification using Transformers</a></li>
  <li><a href="https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks">Community Notebooks</a></li>
</ul>

<h4 id="bertweet-a-pre-trained-language-model-for-english-tweets">BERTweet: A pre-trained language model for English Tweets</h4>
<ul>
  <li><a href="https://github.com/VinAIResearch/BERTweet/blob/master/README.md#models2">Code</a></li>
</ul>

<h4 id="covid-twitter-bert">COVID-Twitter-BERT</h4>
<ul>
  <li><a href="https://github.com/digitalepidemiologylab/covid-twitter-bert">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.07503">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{muller2020covid,
  title={COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter},
  author={M{\"u}ller, Martin and Salath{\'e}, Marcel and Kummervold, Per E},
  journal={arXiv preprint arXiv:2005.07503},
  year={2020}
}
</code></pre></div></div>

<h3 id="gpt2">GPT2</h3>
<h4 id="aitextgen--train-a-gpt-2-text-generating-model-w-gpu">aitextgen – Train a GPT-2 Text-Generating Model w/ GPU</h4>
<ul>
  <li><a href="https://github.com/minimaxir/aitextgen">Code</a></li>
  <li><a href="https://docs.aitextgen.io/">Doc</a></li>
  <li><a href="https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing">Sample</a></li>
  <li><a href="https://twitter.com/minimaxir">Author</a></li>
</ul>

<h3 id="reformers">Reformers</h3>
<p><a href="https://arxiv.org/abs/2001.04451">Colab</a></p>

<h2 id="useful-libs">Useful Libs</h2>
<h3 id="wrapper">Wrapper</h3>
<h4 id="vision">Vision</h4>
<h4 id="text">Text</h4>
<h5 id="fastai-code-first-intro-to-natural-language-processing">fast.ai Code-First Intro to Natural Language Processing</h5>
<ul>
  <li><a href="https://www.fast.ai/2019/07/08/fastai-nlp/">Project Page</a></li>
  <li><a href="https://github.com/fastai/course-nlp">Code</a></li>
</ul>

<p>here is the associated tutorial serie:</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="text-1">Text</h3>
<h4 id="nltk">NLTK</h4>
<ul>
  <li><a href="https://www.nltk.org">Project Page</a></li>
  <li><a href="https://github.com/nltk/nltk">Code</a></li>
  <li><a href="https://www.nltk.org/index.html">Doc</a></li>
  <li><a href="https://cheatography.com/murenei/cheat-sheets/natural-language-processing-with-python-and-nltk/">Cheat Sheet</a></li>
</ul>

<p>I consider <a href="https://twitter.com/Sentdex">@SentDex</a> founder <a href="https://pythonprogramming.net">pythonprogramming.net</a> and <a href="https://www.youtube.com/channel/sentdex">https://www.youtube.com/channel/sentdex</a> as the best tutorial for NLTK</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLI142kNg_e0Q57BmOF9H4UnXiWNSVZZ-O" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="spacy">SpaCy</h4>
<ul>
  <li><a href="https://spacy.io/">Project Page</a></li>
  <li><a href="https://github.com/explosion/spaCy">Code</a></li>
  <li><a href="https://spacy.io/api/doc">Doc</a></li>
  <li><a href="https://www.datacamp.com/community/blog/spacy-cheatsheet">Cheat Sheet</a></li>
  <li><a href="https://twitter.com/spacy_io">Spacy on Twitter @spacy_io</a></li>
  <li><a href="https://www.linkedin.com/company/explosion-ai/">Spacy on Linkedin</a></li>
  <li><a href="https://www.youtube.com/c/ExplosionAI">Spacy on Youtube</a></li>
</ul>

<p>A good Spacy tutorial Youtube serie here :</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLJ39kWiJXSiz1LK8d_fyxb7FTn4mBYOsD" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Spacy channel :</p>

<iframe width="50%" min-height="75%" src="https://www.youtube.com/embed/videoseries?list=PLBmcuObd5An559HbDr_alBnwVsGq-7uTF" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="transformers-huggingface">Transformers (HuggingFace)</h4>
<ul>
  <li><a href="https://huggingface.co/">Project Page</a></li>
  <li><a href="https://github.com/huggingface/transformers">Code</a></li>
  <li><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb">Colab</a></li>
  <li><a href="https://huggingface.co/transformers/">Doc</a></li>
  <li><a href="https://twitter.com/huggingface">HuggingFace on Twitter @HuggingFace</a></li>
  <li><a href="https://www.linkedin.com/company/huggingface/">HuggingFace on Linkedin</a></li>
</ul>

<h4 id="simple-transformers-based-on-huggingface">Simple Transformers (based on HuggingFace)</h4>
<ul>
  <li><a href="https://simpletransformers.ai">Project Page</a></li>
  <li><a href="https://github.com/ThilinaRajapakse/simpletransformers">Code</a></li>
  <li><a href="https://simpletransformers.ai/docs/installation/">Doc</a></li>
  <li><a href="https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3">Blog Post</a></li>
</ul>

<p>Simple Transformers is a wrapper on top of HuggingFace’s Transformer Library take makes it easy to setup and use, here is an example of binary classification :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">simpletransformers.classification</span> <span class="kn">import</span> <span class="n">ClassificationModel</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">transformers_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s">"transformers"</span><span class="p">)</span>
<span class="n">transformers_logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

<span class="c1"># Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.
</span><span class="n">train_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="n">eval_data</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'Example eval sentence belonging to class 1'</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s">'Example eval sentence belonging to class 0'</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">eval_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span>

<span class="c1"># Create a ClassificationModel
</span><span class="n">model</span> <span class="o">=</span> <span class="n">ClassificationModel</span><span class="p">(</span><span class="s">'roberta'</span><span class="p">,</span> <span class="s">'roberta-base'</span><span class="p">)</span> <span class="c1"># You can set class weights by using the optional weight argument
</span>
<span class="c1"># Train the model
</span><span class="n">model</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>

<span class="c1"># Evaluate the model
</span><span class="n">result</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">,</span> <span class="n">wrong_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval_model</span><span class="p">(</span><span class="n">eval_df</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="others">Others</h3>
<h4 id="facebook-mmf">Facebook MMF</h4>
<p>A modular framework for vision &amp; language multimodal research from Facebook AI Research (FAIR)
<img src="https://camo.githubusercontent.com/fa3fcc9fb23c9d5e4ba8a7a822c15d53dc892ef7/68747470733a2f2f692e696d6775722e636f6d2f42503873596e6b2e6a7067" alt="" /></p>
<ul>
  <li><a href="https://github.com/facebookresearch/mmf">Code</a></li>
  <li><a href="https://mmf.readthedocs.io/en/latest/">Doc</a></li>
  <li><a href="http://learningsys.org/nips18/assets/papers/35CameraReadySubmissionPythia___A_platform_for_vision_language_multi_modal_research.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{singh2018pythia,
  title={Pythia-a platform for vision \&amp; language research},
  author={Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  booktitle={SysML Workshop, NeurIPS},
  volume={2018},
  year={2018}
}
</code></pre></div></div>

<h2 id="hands-on">Hands-on</h2>
<h3 id="nlp">NLP</h3>
<ul>
  <li><a href="https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb">Modern NLP in Python</a></li>
</ul>

<p>| Tool | Binary Classification | Multi-Label Classification | Question Answering | Tokenization | Generation | Named Entity Recognition |
|-|-|-|-|-|-|-|</p>

<h4 id="structured-data">Structured Data</h4>
<h3 id="automl">AutoML</h3>
<h4 id="autokeras">AutoKeras</h4>

<p><a href="https://autokeras.com">AutoKeras</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>

<span class="c1"># Prepare the dataset.
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>  <span class="c1"># array([7, 2, 1], dtype=uint8)
</span>
<span class="c1"># Initialize the ImageClassifier.
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="o">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Search for the best model.
</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># Evaluate on the testing data.
</span><span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {accuracy}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">accuracy</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{jin2019auto,
  title={Auto-Keras: An Efficient Neural Architecture Search System},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
  pages={1946--1956},
  year={2019},
  organization={ACM}
}
</code></pre></div></div>

<h4 id="ovhcloud-automl">OVHcloud autoML</h4>
<p><img src="https://labs.ovh.com/sites/default/files/inline-images/upload_source_1.png| width=250" alt="Demo" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/optimization_result_2.png | width=250" alt="Demo2" />
<img src="https://labs.ovh.com/sites/default/files/inline-images/model_kpi_2.png | width=250" alt="Demo3" /></p>

<ul>
  <li><a href="https://labs.ovh.com/machine-learning-platform">Site</a></li>
  <li><a href="https://github.com/ovh/prescience-client">Code</a></li>
  <li><a href="https://gitter.im/ovh/ai">Forum</a></li>
</ul>

<h1 id="deep-learning-use-cases">Deep Learning use cases</h1>

<h2 id="nothing-to-image">Nothing to Image</h2>
<h3 id="generative">Generative</h3>
<h4 id="face">Face</h4>
<h5 id="disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</h5>
<p><img src="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png?raw=true | width=250" alt="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" /></p>
<ul>
  <li><a href="https://github.com/yalharbi/StructuredNoiseInjection">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.12411">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{alharbi2020disentangled,
    title={Disentangled Image Generation Through Structured Noise Injection},
    author={Yazeed Alharbi and Peter Wonka},
    year={2020},
    eprint={2004.12411},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/7h-7wso9E0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="image-to-anything">Image to Anything</h2>

<h3 id="image-to-image">Image to Image</h3>
<h4 id="inpainting">Inpainting</h4>
<h5 id="high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</h5>
<p><img src="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg | width=250" alt="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" /></p>
<ul>
  <li><a href="https://zengxianyu.github.io/iic/">Project Page</a></li>
  <li><a href="http://47.57.135.203:2333/">APP</a></li>
  <li><a href="https://arxiv.org/abs/2005.11742">Paper</a></li>
</ul>

<h5 id="edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</h5>
<p><img src="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png | width=250" alt="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/edge-connect">Code</a></li>
  <li><a href="https://arxiv.org/abs/1901.00212">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  journal={arXiv preprint},
  year={2019},
}
</code></pre></div></div>

<h5 id="progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</h5>
<p><img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_damaged2.png?raw=true | width=250" alt="Before" />
<img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_final2.png?raw=true | width=250" alt="After" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/Inpainting_FRRN">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10478">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{guo2019progressive,
    title={Progressive Image Inpainting with Full-Resolution Residual Network},
    author={Zongyu Guo and Zhibo Chen and Tao Yu and Jiale Chen and Sen Liu},
    year={2019},
    eprint={1907.10478},
    archivePrefix={arXiv},
    primaryClass={eess.IV}
}
</code></pre></div></div>

<h4 id="super-resolution">Super resolution</h4>
<h5 id="pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</h5>
<p><img src="http://pulse.cs.duke.edu/assets/094.jpeg | width=250" alt="http://pulse.cs.duke.edu/assets/094.jpeg" /></p>
<ul>
  <li><a href="PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models">Project Page</a></li>
  <li><a href="https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true">Colab</a></li>
  <li><a href="https://github.com/adamian98/pulse">Code</a></li>
  <li><a href="https://arxiv.org/abs/2003.03808">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{PULSE_CVPR_2020, 
author = {Menon, Sachit and Damian, Alex and Hu, McCourt and Ravi, Nikhil and Rudin, Cynthia}, 
title = {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models}, 
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
month = {June}, 
year = {2020} 
}
</code></pre></div></div>

<h5 id="image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</h5>
<p><img src="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png | width=250" alt="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" /></p>
<ul>
  <li><a href="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.01424">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Mei2020image,
  title={Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining},
  author={Mei, Yiqun and Fan, Yuchen and Zhou, Yuqian and Huang, Lichao and Huang, Thomas S and Shi, Humphrey},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}
</code></pre></div></div>

<h4 id="image-to-image-translation">Image-to-Image translation</h4>
<h5 id="deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</h5>
<p><img src="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg | width=250" alt="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" /></p>
<ul>
  <li><a href="http://geometrylearning.com/DeepFaceDrawing/">Paper</a></li>
</ul>

<iframe width="50%" src="https://www.youtube.com/embed/HSunooUTwKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</h5>
<p><img src="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true | width=250" alt="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/taki0112/UGATIT#paper--official-pytorch-code">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10830">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
</code></pre></div></div>

<h5 id="selfie-to-anime">Selfie to Anime</h5>
<p><img src="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg?raw=true | width=250" alt="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg" /></p>
<ul>
  <li><a href="https://selfie2anime.com/">Project Page</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime">Code</a></li>
  <li><a href="https://market-place.ai.ovh.net/#!/apis/59a0426c-c148-4cff-a042-6cc148fcffa5/pages/06641de1-1b1c-4bd2-a41d-e11b1c3bd230">API</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime/blob/master/assets/Deploying-Models-to-the-Masses.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kim2019ugatit,
    title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
    author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwanghee Lee},
    year={2019},
    eprint={1907.10830},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107">Author’s Site</a></li>
</ul>

<h4 id="segmentation">Segmentation</h4>
<h5 id="poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</h5>
<p><img src="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false | width=250" alt="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" /></p>
<ul>
  <li><a href="https://gitlab.com/irafm-ai/poly-yolo">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.13243">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{hurtik2020polyyolo,
    title={Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3},
    author={Petr Hurtik and Vojtech Molek and Jan Hula and Marek Vajgl and Pavel Vlasanek and Tomas Nejezchleba},
    year={2020},
    eprint={2005.13243},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/2KxNnEV-Zes" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</h5>
<p><img src="https://wukaoliu.github.io/HAttMatting/figures/visualization.png?raw=true | width=250" alt="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" /></p>
<ul>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Project Page</a></li>
  <li><a href="https://github.com/wukaoliu/CVPR2020-HAttMatting">Code</a></li>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Qiao_2020_CVPR,
    author = {Qiao, Yu and Liu, Yuhao and Yang, Xin and Zhou, Dongsheng and Xu, Mingliang and Zhang, Qiang and Wei, Xiaopeng},
    title = {Attention-Guided Hierarchical Structure Aggregation for Image Matting},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</code></pre></div></div>

<h5 id="foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</h5>
<p><img src="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg?raw=true | width=250" alt="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" /></p>
<ul>
  <li><a href="https://github.com/saic-vul/image_harmonization">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.00809">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sofiiuk2020harmonization,
  title={Foreground-aware Semantic Representations for Image Harmonization},
  author={Konstantin Sofiiuk, Polina Popenova, Anton Konushin},
  journal={arXiv preprint arXiv:2006.00809},
  year={2020}
}
</code></pre></div></div>

<h5 id="single-stage-semantic-segmentation-from-image-labels-cvpr-2020">Single-Stage Semantic Segmentation from Image Labels (CVPR 2020)</h5>
<p><img src="https://github.com/visinf/1-stage-wseg/blob/master/figures/results.gif?raw=true | width=250" alt="https://github.com/visinf/1-stage-wseg/blob/master/figures/results.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/visinf/1-stage-wseg">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.08104">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Araslanov:2020:WSEG,
  title     = {Single-Stage Semantic Segmentation from Image Labels},
  author    = {Araslanov, Nikita and and Roth, Stefan},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}
</code></pre></div></div>

<h3 id="others-1">Others</h3>
<h4 id="background-matting-the-world-is-your-green-screen">Background Matting: The World is Your Green Screen</h4>
<p><img src="https://camo.githubusercontent.com/89ad795b21ae7c739811372739e53985b1e7feab/68747470733a2f2f686f6d65732e63732e77617368696e67746f6e2e6564752f7e736f756d796139312f70617065725f7468756d626e61696c732f6d617474696e672e706e67 | width=250" alt="" /></p>
<ul>
  <li><a href="http://grail.cs.washington.edu/projects/background-matting/">Project Page</a></li>
  <li><a href="https://github.com/senguptaumd/Background-Matting">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.00626">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{BMSengupta20,
  title={Background Matting: The World is Your Green Screen},
  author = {Soumyadip Sengupta and Vivek Jayaram and Brian Curless and Steve Seitz and Ira Kemelmacher-Shlizerman},
  booktitle={Computer Vision and Pattern Regognition (CVPR)},
  year={2020}
}
</code></pre></div></div>

<ul>
  <li><a href="https://link.medium.com/suCEIzEed7">Blog Post</a></li>
</ul>

<h4 id="3d-photography-using-context-aware-layered-depth-inpainting-cvpr-2020">3D Photography using Context-aware Layered Depth Inpainting (CVPR 2020)</h4>
<p><img src="https://camo.githubusercontent.com/8dd5b529c99cdfcedd043c8239b68c4d7a23a148/68747470733a2f2f66696c65626f782e6563652e76742e6564752f7e6a626875616e672f70726f6a6563742f334450686f746f2f334450686f746f5f7465617365722e6a7067 | width=250" alt="" /></p>
<ul>
  <li><a href="https://shihmengli.github.io/3D-Photo-Inpainting/">Project Page</a></li>
  <li><a href="https://github.com/vt-vl-lab/3d-photo-inpainting/blob/master/README.md">Code</a></li>
  <li><a href="https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz">Colab</a></li>
  <li><a href="https://arxiv.org/abs/2004.04727">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Shih3DP20,
  author = {Shih, Meng-Li and Su, Shih-Yang and Kopf, Johannes and Huang, Jia-Bin},
  title = {3D Photography using Context-aware Layered Depth Inpainting},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}
</code></pre></div></div>

<h4 id="project-an-image-centroid-to-another-image-using-opencv">Project an image centroid to another image using OpenCV</h4>
<p><img src="https://github.com/cyrildiagne/screenpoint/blob/master/example/match_debug.png?raw=true" alt="https://github.com/cyrildiagne/screenpoint/blob/master/example/match_debug.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/cyrildiagne/screenpoint/blob/master/README.md">Code</a>
####</li>
</ul>

<h3 id="image-to-text">Image to Text</h3>
<h5 id="yolov4-optimal-speed-and-accuracy-of-object-detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</h5>
<p><img src="https://i.ibb.co/mz376Rd/Image-PNG.png" alt="" /></p>
<ul>
  <li><a href="https://github.com/AlexeyAB/darknet">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.10934">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{bochkovskiy2020yolov4,
    title={YOLOv4: Optimal Speed and Accuracy of Object Detection},
    author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
    year={2020},
    eprint={2004.10934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<h5 id="image-captioning-with-pytorch">Image Captioning with PyTorch</h5>
<p><img src="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg | width=250" alt="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" /></p>
<ul>
  <li><a href="https://github.com/jayeshsaita/image_captioning_pytorch">Code</a></li>
</ul>

<h5 id="resnest-split-attention-networks">ResNeSt: Split-Attention Networks</h5>
<p><img src="https://raw.githubusercontent.com/zhanghang1989/ResNeSt/master/miscs/abstract.jpg" alt="" /></p>
<ul>
  <li><a href="https://github.com/zhanghang1989/ResNeSt">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.08955">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2020resnest,
title={ResNeSt: Split-Attention Networks},
author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Muller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
journal={arXiv preprint arXiv:2004.08955},
year={2020}
}
</code></pre></div></div>

<h3 id="image-to-soundspeech">Image to Sound/Speech</h3>

<h3 id="image-to-video">Image to Video</h3>

<h2 id="text-to-anything">Text to Anything</h2>

<h3 id="text-to-image">Text to Image</h3>

<h3 id="text-to-text">Text to Text</h3>
<h4 id="text-generation">Text Generation</h4>
<h5 id="lyrics-generation">Lyrics Generation</h5>
<ul>
  <li><a href="https://colab.research.google.com/drive/12g07FS2WkNctNy_bYb7a5ZNFAsJcN0dz?usp=sharing">Colab</a></li>
  <li><a href="https://eilab.gatech.edu/mark-riedl">Author</a></li>
</ul>

<h5 id="next-word-prediction">Next Word Prediction</h5>
<p><img src="https://raw.githubusercontent.com/renatoviolin/next_word_prediction/master/word_prediction.gif =250x" alt="UI" /></p>
<ul>
  <li><a href="https://github.com/renatoviolin/next_word_prediction">Code</a></li>
</ul>

<h4 id="code-to-code">Code to Code</h4>
<h5 id="unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</h5>
<ul>
  <li><a href="https://arxiv.org/abs/2006.03511">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{lachaux2020unsupervised,
    title={Unsupervised Translation of Programming Languages},
    author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
    year={2020},
    eprint={2006.03511},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/xTzFJIknh7E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="text-to-soundspeech">Text to Sound/Speech</h3>
<h4 id="pitchtron-towards-audiobook-generation-from-ordinary-peoples-voices">Pitchtron: Towards audiobook generation from ordinary people’s voices</h4>
<ul>
  <li><a href="https://sunghee.kaist.ac.kr/entry/pitchtron">Project Page</a></li>
  <li><a href="https://arxiv.org/abs/2005.10456">Paper</a></li>
  <li><a href="https://github.com/hash2430/pitchtron">Code</a></li>
  <li><a href="https://sunghee.kaist.ac.kr/entry/pitchtron">Samples</a></li>
</ul>

<audio controls="controls">
    <source src="https://www.dropbox.com/s/g9ch0aw2wbgmwek/hard_emotive_1.wav?dl=1" type="audio/wav" />&lt;/source&gt;
    <p>Your browser does not support the audio element.</p>
</audio>

<h4 id="transformers-tts">Transformers TTS</h4>
<ul>
  <li><a href="https://as-ideas.github.io/TransformerTTS/">Project Page</a></li>
  <li><a href="https://github.com/as-ideas/TransformerTTS">Code</a></li>
  <li><a href="https://as-ideas.github.io/TransformerTTS/">Samples</a></li>
</ul>

<audio controls="controls">
    <source src="https://github.com/as-ideas/tts_model_outputs/blob/master/ljspeech_transformertts/Trump.wav?raw=true" type="audio/wav" />&lt;/source&gt;
    <p>Your browser does not support the audio element.</p>
</audio>

<h3 id="text-to-video">Text to Video</h3>

<h2 id="soundspeech-to-anything">Sound/Speech to Anything</h2>

<h3 id="soundspeech-to-image">Sound/Speech to Image</h3>
<h5 id="audio-to-image-conversion">Audio to Image Conversion</h5>
<ul>
  <li><a href="https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda">Code</a></li>
</ul>

<h3 id="soundspeech-to-text">Sound/Speech to Text</h3>
<h5 id="speech-command-recognition">Speech Command Recognition</h5>
<ul>
  <li><a href="https://github.com/jayeshsaita/Speech-Commands-Recognition">Code</a></li>
  <li><a href="https://www.linkedin.com/in/jayeshsaita">Author</a></li>
</ul>

<h3 id="soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</h3>
<h4 id="speaker-independent-emotional-voice-conversion-based-on-conditional-vaw-gan-and-cwt">Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT</h4>
<ul>
  <li><a href="https://kunzhou9646.github.io/speaker-independent-emotional-vc/">Project Page</a></li>
  <li><a href="https://github.com/KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT">Code</a></li>
  <li><a href="https://www.researchgate.net/publication/341388058_Converting_Anyone's_Emotion_Towards_Speaker-Independent_Emotional_Voice_Conversion">Paper</a></li>
  <li><a href="https://kunzhou9646.github.io/speaker-independent-emotional-vc/">Sample</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@unknown{unknown,
author = {Zhou, Kun and Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
year = {2020},
month = {05},
pages = {},
title = {Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion},
doi = {10.13140/RG.2.2.20921.60006}
}
</code></pre></div></div>

<h3 id="soundspeech-to-video">Sound/Speech to Video</h3>

<h2 id="video-to-anything">Video to Anything</h2>

<h3 id="video-to-video">Video to Video</h3>
<h4 id="segmentation-1">Segmentation</h4>
<h5 id="mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</h5>
<p><img src="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif | width=250" alt="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" /></p>
<ul>
  <li><a href="https://github.com/mseg-dataset">Code</a></li>
  <li><a href="https://vladlen.info/papers/MSeg.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{MSeg_2020_CVPR,
author = {Lambert, John and Zhuang, Liu and Sener, Ozan and Hays, James and Koltun, Vladlen},
title = {MSeg A Composite Dataset for Multi-domain Semantic Segmentation},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
year = {2020}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/PzBK6K5gyyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</h5>
<p><img src="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true | width=250" alt="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/AliaksandrSiarohin/motion-cosegmentation">Code</a></li>
  <li><a href="http://arxiv.org/abs/2004.03234">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Siarohin_2020_motion,
  title={Motion Supervised co-part Segmentation},
  author={Siarohin, Aliaksandr and Roy, Subhankar and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={arXiv preprint},
  year={2020}
}
</code></pre></div></div>

<iframe width="50%" src="https://www.youtube.com/embed/RJ4Nj1wV5iA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="video-to-image">Video to Image</h3>

<h3 id="video-to-text">Video to Text</h3>

<h3 id="video-tosoundspeech">Video toSound/Speech</h3>

<h3 id="video-to-video-1">Video to Video</h3>

<h1 id="inference">Inference</h1>
<h2 id="fastai">Fastai</h2>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce <a href="https://twitter.com/hashtag/fastinference?src=hash&amp;ref_src=twsrc%5Etfw">#fastinference</a>! The bad news: fastshap and ClassConfusion are gone. The good news: they moved to a new home! In this module we have all of the above plus some speed-up and QOL integrations into <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>&#39;s inference methods, see here: 1/<a href="https://t.co/SLgJahtSr5">https://t.co/SLgJahtSr5</a> <a href="https://t.co/1oFkMe4SsP">pic.twitter.com/1oFkMe4SsP</a></p>&mdash; Zach #masks4all Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1269818072577331200?ref_src=twsrc%5Etfw">June 8, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://jianjye.com/p/deploy-fastai-digitalocean-ubuntu-flask-supervisor/">How to deploy Fastai on Ubuntu</a></p>

<h2 id="huggingface">HuggingFace</h2>
<ul>
  <li><a href="https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18">Deploying HuggingFace to Production</a></li>
  <li><a href="https://github.com/huggingface/transformers/blob/master/examples/movement-pruning/README.md">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sanh2020movement,
    title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
    author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
    year={2020},
    eprint={2005.07683},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<h2 id="hummingbird">Hummingbird</h2>
<p>python library that compiles trained ML models into tensor computation for faster inference. Supported models include sklearn decision trees, random forest, lightgbm, xgboost.</p>

<ul>
  <li><a href="https://github.com/microsoft/hummingbird">Code</a></li>
  <li><a href="https://github.com/microsoft/hummingbird/tree/master/notebooks">Examples</a></li>
</ul>

<h1 id="tools">Tools</h1>
<h2 id="terminal">Terminal</h2>
<h3 id="rich">Rich</h3>
<p>Rich is a Python library for rich text and beautiful formatting in the terminal
<img src="https://github.com/willmcgugan/rich/raw/master/imgs/features.png?raw=true" alt="https://github.com/willmcgugan/rich/raw/master/imgs/features.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/willmcgugan/rich">Code</a></li>
  <li><a href="https://rich.readthedocs.io/en/latest/">Doc</a></li>
</ul>

<h2 id="python">Python</h2>
<h3 id="pyaudio-fft">PyAudio FFT</h3>
<p><img src="https://raw.githubusercontent.com/tr1pzz/Realtime_PyAudio_FFT/master/assets/teaser.gif" alt="https://raw.githubusercontent.com/tr1pzz/Realtime_PyAudio_FFT/master/assets/teaser.gif" /></p>
<ul>
  <li><a href="https://github.com/tr1pzz/Realtime_PyAudio_FFT">https://github.com/tr1pzz/Realtime_PyAudio_FFT</a></li>
</ul>

<h1 id="cool-projects">Cool projects</h1>
<h2 id="web-based">Web based</h2>
<ul>
  <li><a href="https://teachablemachine.withgoogle.com/">Teachable Machine</a></li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/databuzzword/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/databuzzword/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/databuzzword/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jqueguiner" title="jqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://stackoverflow.com/users/users%2F786021%2Fguignol" title="users/786021/guignol"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#stackoverflow"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/jlqueguiner" title="jlqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jilijeanlouis" title="jilijeanlouis"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://www.youtube.com/UCwPhv5c2FQf6Ai3E6rT8_cw" title="UCwPhv5c2FQf6Ai3E6rT8_cw"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#youtube"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
