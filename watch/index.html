<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/databuzzword/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/databuzzword/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Technology Watch | DataBuzzWord Blog!</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Technology Watch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<meta property="og:description" content="This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤" />
<link rel="canonical" href="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:url" content="https://jqueguiner.github.io/databuzzword/watch/" />
<meta property="og:site_name" content="DataBuzzWord Blog!" />
<script type="application/ld+json">
{"description":"This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤","@type":"WebPage","url":"https://jqueguiner.github.io/databuzzword/watch/","headline":"Technology Watch","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://jqueguiner.github.io/databuzzword/feed.xml" title="DataBuzzWord Blog!" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-128190943-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/databuzzword/">DataBuzzWord Blog!</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/databuzzword/about/">About The Podcast</a><a class="page-link" href="/databuzzword/experimenta/">DataBuzzWord Experiments</a><a class="page-link" href="/databuzzword/reading-list/">Reading List</a><a class="page-link" href="/databuzzword/search/">Search</a><a class="page-link" href="/databuzzword/categories/">Tags</a><a class="page-link" href="/databuzzword/watch/">Technology Watch</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Technology Watch</h1>
  </header>

  <div class="post-content">
    <h1 id="table-of-content">Table of Content</h1>

<ol id="markdown-toc">
  <li><a href="#table-of-content" id="markdown-toc-table-of-content">Table of Content</a></li>
  <li><a href="#machinedeep-learning-tools" id="markdown-toc-machinedeep-learning-tools">Machine/Deep Learning tools</a>    <ol>
      <li><a href="#technics" id="markdown-toc-technics">Technics</a>        <ol>
          <li><a href="#transformers" id="markdown-toc-transformers">Transformers</a></li>
          <li><a href="#bert" id="markdown-toc-bert">BERT</a></li>
          <li><a href="#bart" id="markdown-toc-bart">BART</a></li>
          <li><a href="#xlnet" id="markdown-toc-xlnet">XLNet</a></li>
          <li><a href="#gpt" id="markdown-toc-gpt">GPT</a></li>
          <li><a href="#gpt2" id="markdown-toc-gpt2">GPT2</a></li>
          <li><a href="#gpt3" id="markdown-toc-gpt3">GPT3</a></li>
          <li><a href="#reformers" id="markdown-toc-reformers">Reformers</a></li>
        </ol>
      </li>
      <li><a href="#useful-libs" id="markdown-toc-useful-libs">Useful Libs</a>        <ol>
          <li><a href="#wrappers" id="markdown-toc-wrappers">Wrappers</a>            <ol>
              <li><a href="#vision" id="markdown-toc-vision">Vision</a></li>
              <li><a href="#text" id="markdown-toc-text">Text</a></li>
              <li><a href="#structured-data" id="markdown-toc-structured-data">Structured Data</a></li>
            </ol>
          </li>
          <li><a href="#automl" id="markdown-toc-automl">AutoMl</a>            <ol>
              <li><a href="#autokeras" id="markdown-toc-autokeras">AutoKeras</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#deep-learning-use-cases" id="markdown-toc-deep-learning-use-cases">Deep Learning use cases</a>    <ol>
      <li><a href="#nothing-to-image" id="markdown-toc-nothing-to-image">Nothing to Image</a>        <ol>
          <li><a href="#generative" id="markdown-toc-generative">Generative</a>            <ol>
              <li><a href="#face" id="markdown-toc-face">Face</a>                <ol>
                  <li><a href="#disentangled-image-generation-through-structured-noise-injection" id="markdown-toc-disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</a></li>
                </ol>
              </li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#image-to-anything" id="markdown-toc-image-to-anything">Image to Anything</a>        <ol>
          <li><a href="#image-to-image" id="markdown-toc-image-to-image">Image to Image</a>            <ol>
              <li><a href="#inpainting" id="markdown-toc-inpainting">Inpainting</a>                <ol>
                  <li><a href="#high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling" id="markdown-toc-high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</a></li>
                  <li><a href="#edgeconnect-generative-image-inpainting-with-adversarial-edge-learning" id="markdown-toc-edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a></li>
                  <li><a href="#progressive-image-inpainting-with-full-resolution-residual-network" id="markdown-toc-progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</a></li>
                </ol>
              </li>
              <li><a href="#super-resolution" id="markdown-toc-super-resolution">Super resolution</a>                <ol>
                  <li><a href="#pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models" id="markdown-toc-pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</a></li>
                  <li><a href="#image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining" id="markdown-toc-image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</a></li>
                </ol>
              </li>
              <li><a href="#image-to-image-translation" id="markdown-toc-image-to-image-translation">Image-to-Image translation</a>                <ol>
                  <li><a href="#deepfacedrawing-deep-generation-of-face-images-from-sketches" id="markdown-toc-deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</a></li>
                  <li><a href="#ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020" id="markdown-toc-ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</a></li>
                  <li><a href="#selfie-to-anime" id="markdown-toc-selfie-to-anime">Selfie to Anime</a></li>
                </ol>
              </li>
              <li><a href="#segmentation" id="markdown-toc-segmentation">Segmentation</a>                <ol>
                  <li><a href="#poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3" id="markdown-toc-poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</a></li>
                  <li><a href="#attention-guided-hierarchical-structure-aggregation-for-image-matting" id="markdown-toc-attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</a></li>
                  <li><a href="#foreground-aware-semantic-representations-for-image-harmonization" id="markdown-toc-foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#image-to-text" id="markdown-toc-image-to-text">Image to Text</a>            <ol>
              <li><a href="#image-captioning" id="markdown-toc-image-captioning">Image Captioning</a></li>
            </ol>
          </li>
          <li><a href="#image-to-soundspeech" id="markdown-toc-image-to-soundspeech">Image to Sound/Speech</a></li>
          <li><a href="#image-to-video" id="markdown-toc-image-to-video">Image to Video</a></li>
        </ol>
      </li>
      <li><a href="#text-to-anything" id="markdown-toc-text-to-anything">Text to Anything</a>        <ol>
          <li><a href="#text-to-image" id="markdown-toc-text-to-image">Text to Image</a></li>
          <li><a href="#text-to-text" id="markdown-toc-text-to-text">Text to Text</a>            <ol>
              <li><a href="#text-generation" id="markdown-toc-text-generation">Text Generation</a>                <ol>
                  <li><a href="#next-word-prediction" id="markdown-toc-next-word-prediction">Next Word Prediction</a></li>
                </ol>
              </li>
              <li><a href="#code-to-code" id="markdown-toc-code-to-code">Code to Code</a>                <ol>
                  <li><a href="#unsupervised-translation-of-programming-languages" id="markdown-toc-unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#text-to-soundspeech" id="markdown-toc-text-to-soundspeech">Text to Sound/Speech</a></li>
          <li><a href="#text-to-video" id="markdown-toc-text-to-video">Text to Video</a></li>
        </ol>
      </li>
      <li><a href="#soundspeech-to-anything" id="markdown-toc-soundspeech-to-anything">Sound/Speech to Anything</a>        <ol>
          <li><a href="#soundspeech-to-image" id="markdown-toc-soundspeech-to-image">Sound/Speech to Image</a>            <ol>
              <li><a href="#audio-to-image-conversion" id="markdown-toc-audio-to-image-conversion">Audio to Image Conversion</a></li>
            </ol>
          </li>
          <li><a href="#soundspeech-to-text" id="markdown-toc-soundspeech-to-text">Sound/Speech to Text</a>            <ol>
              <li><a href="#speech-command-reognitiob" id="markdown-toc-speech-command-reognitiob">Speech Command Reognitiob</a></li>
            </ol>
          </li>
          <li><a href="#soundspeech-to-soundspeech" id="markdown-toc-soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</a></li>
          <li><a href="#soundspeech-to-video" id="markdown-toc-soundspeech-to-video">Sound/Speech to Video</a></li>
        </ol>
      </li>
      <li><a href="#video-to-anything" id="markdown-toc-video-to-anything">Video to Anything</a>        <ol>
          <li><a href="#video-to-video" id="markdown-toc-video-to-video">Video to Video</a>            <ol>
              <li><a href="#segmentation-1" id="markdown-toc-segmentation-1">Segmentation</a>                <ol>
                  <li><a href="#mseg--a-composite-dataset-for-multi-domain-semantic-segmentation" id="markdown-toc-mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</a></li>
                  <li><a href="#motion-supervised-co-part-segmentation" id="markdown-toc-motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</a></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><a href="#video-to-image" id="markdown-toc-video-to-image">Video to Image</a></li>
          <li><a href="#video-to-text" id="markdown-toc-video-to-text">Video to Text</a></li>
          <li><a href="#video-tosoundspeech" id="markdown-toc-video-tosoundspeech">Video toSound/Speech</a></li>
          <li><a href="#video-to-video-1" id="markdown-toc-video-to-video-1">Video to Video</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#inference" id="markdown-toc-inference">Inference</a>    <ol>
      <li><a href="#fastai" id="markdown-toc-fastai">Fastai</a></li>
      <li><a href="#huggingface" id="markdown-toc-huggingface">HuggingFace</a></li>
    </ol>
  </li>
</ol>

<h1 id="machinedeep-learning-tools">Machine/Deep Learning tools</h1>
<h2 id="technics">Technics</h2>
<h3 id="transformers">Transformers</h3>
<ul>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">The illustrated Transformers</a></li>
  <li><a href="https://arxiv.org/abs/2001.04451">Transformers explained</a></li>
  <li><a href="https://arxiv.org/abs/1706.03762">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<h3 id="bert">BERT</h3>
<h3 id="bart">BART</h3>
<h3 id="xlnet">XLNet</h3>
<h3 id="gpt">GPT</h3>
<h3 id="gpt2">GPT2</h3>
<h3 id="gpt3">GPT3</h3>

<h3 id="reformers">Reformers</h3>
<p><a href="https://arxiv.org/abs/2001.04451">Colab</a></p>

<h2 id="useful-libs">Useful Libs</h2>
<h3 id="wrappers">Wrappers</h3>
<h4 id="vision">Vision</h4>
<h4 id="text">Text</h4>

<p>| Tool                                             | Binary Classification | Multi-Label Classification | Question Answering | Tokenization | Generation | Named Entity Recognition |
|————————————————–|———————–|—————————-|——————–|————–|————|————————–|
| <a href="https://ohmeow.github.io/blurr/#Imports">Blurr</a> |                       |                            |                    |              |            |                          |
|                                                  |                       |                            |                    |              |            |                          |
|                                                  |                       |                            |                    |              |            |                          |</p>
<h4 id="structured-data">Structured Data</h4>
<h3 id="automl">AutoMl</h3>
<h4 id="autokeras">AutoKeras</h4>

<p><a href="https://autokeras.com">AutoKeras</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>

<span class="c1"># Prepare the dataset.
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000, 28, 28)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (60000,)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>  <span class="c1"># array([7, 2, 1], dtype=uint8)
</span>
<span class="c1"># Initialize the ImageClassifier.
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="o">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Search for the best model.
</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># Evaluate on the testing data.
</span><span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {accuracy}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">accuracy</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{jin2019auto,
  title={Auto-Keras: An Efficient Neural Architecture Search System},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
  pages={1946--1956},
  year={2019},
  organization={ACM}
}
</code></pre></div></div>
<h1 id="deep-learning-use-cases">Deep Learning use cases</h1>

<h2 id="nothing-to-image">Nothing to Image</h2>
<h3 id="generative">Generative</h3>
<h4 id="face">Face</h4>
<h5 id="disentangled-image-generation-through-structured-noise-injection">Disentangled Image Generation Through Structured Noise Injection</h5>
<p><img src="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" alt="https://github.com/yalharbi/StructuredNoiseInjection/raw/master/example_fakes_alllocal.png" /></p>
<ul>
  <li><a href="https://github.com/yalharbi/StructuredNoiseInjection">Code</a></li>
  <li><a href="https://arxiv.org/abs/2004.12411">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{alharbi2020disentangled,
    title={Disentangled Image Generation Through Structured Noise Injection},
    author={Yazeed Alharbi and Peter Wonka},
    year={2020},
    eprint={2004.12411},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/7h-7wso9E0k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="image-to-anything">Image to Anything</h2>

<h3 id="image-to-image">Image to Image</h3>
<h4 id="inpainting">Inpainting</h4>
<h5 id="high-resolution-image-inpainting-with-iterative-confidence-feedback-and-guided-upsampling">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</h5>
<p><img src="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" alt="https://s1.ax1x.com/2020/03/18/8wQG5T.jpg" /></p>
<ul>
  <li><a href="https://zengxianyu.github.io/iic/">Project Page</a></li>
  <li><a href="http://47.57.135.203:2333/">APP</a></li>
  <li><a href="https://arxiv.org/abs/2005.11742">Paper</a></li>
</ul>

<h5 id="edgeconnect-generative-image-inpainting-with-adversarial-edge-learning">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</h5>
<p><img src="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" alt="https://user-images.githubusercontent.com/1743048/50673917-aac15080-0faf-11e9-9100-ef10864087c8.png" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/edge-connect">Code</a></li>
  <li><a href="https://arxiv.org/abs/1901.00212">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  journal={arXiv preprint},
  year={2019},
}
</code></pre></div></div>

<h5 id="progressive-image-inpainting-with-full-resolution-residual-network">Progressive Image Inpainting with Full-Resolution Residual Network</h5>
<p><img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_damaged2.png?raw=true" alt="Before" />
<img src="https://github.com/zengxianyu/Inpainting_FRRN/blob/master/examples/ex_final2.png?raw=true" alt="After" /></p>
<ul>
  <li><a href="https://github.com/zengxianyu/Inpainting_FRRN">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10478">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{guo2019progressive,
    title={Progressive Image Inpainting with Full-Resolution Residual Network},
    author={Zongyu Guo and Zhibo Chen and Tao Yu and Jiale Chen and Sen Liu},
    year={2019},
    eprint={1907.10478},
    archivePrefix={arXiv},
    primaryClass={eess.IV}
}
</code></pre></div></div>

<h4 id="super-resolution">Super resolution</h4>
<h5 id="pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models">PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</h5>
<p><img src="http://pulse.cs.duke.edu/assets/094.jpeg" alt="http://pulse.cs.duke.edu/assets/094.jpeg" /></p>
<ul>
  <li><a href="PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models">Project Page</a></li>
  <li><a href="https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true">Colab</a></li>
  <li><a href="https://github.com/adamian98/pulse">Code</a></li>
  <li><a href="https://arxiv.org/abs/2003.03808">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{PULSE_CVPR_2020, 
author = {Menon, Sachit and Damian, Alex and Hu, McCourt and Ravi, Nikhil and Rudin, Cynthia}, 
title = {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models}, 
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
month = {June}, 
year = {2020} 
}
</code></pre></div></div>

<h5 id="image-super-resolution-with-cross-scale-non-local-attention-and-exhaustive-self-exemplars-mining">Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining</h5>
<p><img src="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" alt="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/raw/master/Figs/Visual_3.png" /></p>
<ul>
  <li><a href="https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.01424">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Mei2020image,
  title={Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining},
  author={Mei, Yiqun and Fan, Yuchen and Zhou, Yuqian and Huang, Lichao and Huang, Thomas S and Shi, Humphrey},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}
</code></pre></div></div>

<h4 id="image-to-image-translation">Image-to-Image translation</h4>
<h5 id="deepfacedrawing-deep-generation-of-face-images-from-sketches">DeepFaceDrawing: Deep Generation of Face Images from Sketches</h5>
<p><img src="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" alt="http://geometrylearning.com/DeepFaceDrawing/imgs/teaser.jpg" /></p>
<ul>
  <li><a href="http://geometrylearning.com/DeepFaceDrawing/">Paper</a></li>
</ul>

<iframe width="100%" src="https://www.youtube.com/embed/HSunooUTwKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="ugatit-unsupervised-generative-attentional-networks-with-adaptive-layer-instance-normalization-for-image-to-image-translation-iclr-2020">UGATIT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020)</h5>
<p><img src="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" alt="https://github.com/taki0112/UGATIT/blob/master/assets/teaser.png?raw=true" /></p>
<ul>
  <li><a href="https://github.com/taki0112/UGATIT#paper--official-pytorch-code">Code</a></li>
  <li><a href="https://arxiv.org/abs/1907.10830">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
</code></pre></div></div>

<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107€">Author’s Site</a></li>
</ul>

<h5 id="selfie-to-anime">Selfie to Anime</h5>
<p><img src="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg?raw=true" alt="https://github.com/jqueguiner/databuzzword/blob/master/images/A578852A-9A4D-4D90-88E0-A4D81C7D41B3.jpeg" /></p>
<ul>
  <li><a href="https://selfie2anime.com/">Project Page</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime">Code</a></li>
  <li><a href="https://market-place.ai.ovh.net/#!/apis/59a0426c-c148-4cff-a042-6cc148fcffa5/pages/06641de1-1b1c-4bd2-a41d-e11b1c3bd230">API</a></li>
  <li><a href="https://github.com/t04glovern/selfie2anime/blob/master/assets/Deploying-Models-to-the-Masses.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kim2019ugatit,
    title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
    author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwanghee Lee},
    year={2019},
    eprint={1907.10830},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>
<ul>
  <li><a href="https://www.notion.so/Make-everyone-s-life-more-fun-using-AI-b15459d868bb490184e256cd95f26107">Author’s Site</a></li>
</ul>

<h4 id="segmentation">Segmentation</h4>
<h5 id="poly-yolo-higher-speed-more-precise-detection-and-instance-segmentation-for-yolov3">Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</h5>
<p><img src="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" alt="https://gitlab.com/irafm-ai/poly-yolo/-/raw/master/poly-yolo-titlepage-image.jpg?inline=false" /></p>
<ul>
  <li><a href="https://gitlab.com/irafm-ai/poly-yolo">Code</a></li>
  <li><a href="https://arxiv.org/abs/2005.13243">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{hurtik2020polyyolo,
    title={Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3},
    author={Petr Hurtik and Vojtech Molek and Jan Hula and Marek Vajgl and Pavel Vlasanek and Tomas Nejezchleba},
    year={2020},
    eprint={2005.13243},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/2KxNnEV-Zes" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="attention-guided-hierarchical-structure-aggregation-for-image-matting">Attention-Guided Hierarchical Structure Aggregation for Image Matting</h5>
<p><img src="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" alt="https://wukaoliu.github.io/HAttMatting/figures/visualization.png" /></p>
<ul>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Project Page</a></li>
  <li><a href="https://github.com/wukaoliu/CVPR2020-HAttMatting">Code</a></li>
  <li><a href="https://wukaoliu.github.io/HAttMatting/">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Qiao_2020_CVPR,
    author = {Qiao, Yu and Liu, Yuhao and Yang, Xin and Zhou, Dongsheng and Xu, Mingliang and Zhang, Qiang and Wei, Xiaopeng},
    title = {Attention-Guided Hierarchical Structure Aggregation for Image Matting},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</code></pre></div></div>

<h5 id="foreground-aware-semantic-representations-for-image-harmonization">Foreground-aware Semantic Representations for Image Harmonization</h5>
<p><img src="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" alt="https://github.com/saic-vul/image_harmonization/raw/master/images/ih_teaser.jpg" /></p>
<ul>
  <li><a href="https://github.com/saic-vul/image_harmonization">Code</a></li>
  <li><a href="https://arxiv.org/abs/2006.00809">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{sofiiuk2020harmonization,
  title={Foreground-aware Semantic Representations for Image Harmonization},
  author={Konstantin Sofiiuk, Polina Popenova, Anton Konushin},
  journal={arXiv preprint arXiv:2006.00809},
  year={2020}
}
</code></pre></div></div>

<h3 id="image-to-text">Image to Text</h3>
<h5 id="image-captioning">Image Captioning</h5>
<p><img src="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" alt="https://raw.githubusercontent.com/jayeshsaita/image_captioning_pytorch/master/data/sample_output/output_6774537791.jpg" /></p>
<ul>
  <li><a href="https://github.com/jayeshsaita/image_captioning_pytorch">Code</a></li>
</ul>

<h3 id="image-to-soundspeech">Image to Sound/Speech</h3>

<h3 id="image-to-video">Image to Video</h3>

<h2 id="text-to-anything">Text to Anything</h2>

<h3 id="text-to-image">Text to Image</h3>

<h3 id="text-to-text">Text to Text</h3>
<h4 id="text-generation">Text Generation</h4>
<h5 id="next-word-prediction">Next Word Prediction</h5>
<p><img src="https://raw.githubusercontent.com/renatoviolin/next_word_prediction/master/word_prediction.gif" alt="UI" /></p>
<ul>
  <li><a href="https://github.com/renatoviolin/next_word_prediction">Code</a></li>
</ul>

<h4 id="code-to-code">Code to Code</h4>
<h5 id="unsupervised-translation-of-programming-languages">Unsupervised Translation of Programming Languages</h5>
<ul>
  <li><a href="https://arxiv.org/abs/2006.03511">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{lachaux2020unsupervised,
    title={Unsupervised Translation of Programming Languages},
    author={Marie-Anne Lachaux and Baptiste Roziere and Lowik Chanussot and Guillaume Lample},
    year={2020},
    eprint={2006.03511},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/xTzFJIknh7E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<h3 id="text-to-soundspeech">Text to Sound/Speech</h3>

<h3 id="text-to-video">Text to Video</h3>

<h2 id="soundspeech-to-anything">Sound/Speech to Anything</h2>

<h3 id="soundspeech-to-image">Sound/Speech to Image</h3>
<h5 id="audio-to-image-conversion">Audio to Image Conversion</h5>
<ul>
  <li><a href="https://www.kaggle.com/timolee/audio-data-conversion-to-images-eda">Code</a></li>
</ul>

<h3 id="soundspeech-to-text">Sound/Speech to Text</h3>
<h5 id="speech-command-reognitiob">Speech Command Reognitiob</h5>
<ul>
  <li><a href="https://github.com/jayeshsaita/Speech-Commands-Recognition">Code</a></li>
  <li><a href="https://www.linkedin.com/in/jayeshsaita">Paper</a></li>
</ul>

<h3 id="soundspeech-to-soundspeech">Sound/Speech to Sound/Speech</h3>

<h3 id="soundspeech-to-video">Sound/Speech to Video</h3>

<h2 id="video-to-anything">Video to Anything</h2>

<h3 id="video-to-video">Video to Video</h3>
<h4 id="segmentation-1">Segmentation</h4>
<h5 id="mseg--a-composite-dataset-for-multi-domain-semantic-segmentation">MSeg : A Composite Dataset for Multi-domain Semantic Segmentation</h5>
<p><img src="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" alt="https://user-images.githubusercontent.com/62491525/83893958-abb75e00-a71e-11ea-978c-ab4080b4e718.gif" /></p>
<ul>
  <li><a href="https://github.com/mseg-dataset">Code</a></li>
  <li><a href="https://vladlen.info/papers/MSeg.pdf">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{MSeg_2020_CVPR,
author = {Lambert, John and Zhuang, Liu and Sener, Ozan and Hays, James and Koltun, Vladlen},
title = {MSeg A Composite Dataset for Multi-domain Semantic Segmentation},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
year = {2020}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/PzBK6K5gyyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h5 id="motion-supervised-co-part-segmentation">Motion Supervised co-part Segmentation</h5>
<p><img src="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" alt="https://github.com/AliaksandrSiarohin/motion-cosegmentation/blob/master/sup-mat/beard-line.gif?raw=true" /></p>
<ul>
  <li><a href="https://github.com/AliaksandrSiarohin/motion-cosegmentation">Code</a></li>
  <li><a href="http://arxiv.org/abs/2004.03234">Paper</a></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Siarohin_2020_motion,
  title={Motion Supervised co-part Segmentation},
  author={Siarohin, Aliaksandr and Roy, Subhankar and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={arXiv preprint},
  year={2020}
}
</code></pre></div></div>

<iframe width="100%" src="https://www.youtube.com/embed/RJ4Nj1wV5iA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h3 id="video-to-image">Video to Image</h3>

<h3 id="video-to-text">Video to Text</h3>

<h3 id="video-tosoundspeech">Video toSound/Speech</h3>

<h3 id="video-to-video-1">Video to Video</h3>

<h1 id="inference">Inference</h1>
<h2 id="fastai">Fastai</h2>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Happy to announce <a href="https://twitter.com/hashtag/fastinference?src=hash&amp;ref_src=twsrc%5Etfw">#fastinference</a>! The bad news: fastshap and ClassConfusion are gone. The good news: they moved to a new home! In this module we have all of the above plus some speed-up and QOL integrations into <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw">@fastdotai</a>&#39;s inference methods, see here: 1/<a href="https://t.co/SLgJahtSr5">https://t.co/SLgJahtSr5</a> <a href="https://t.co/1oFkMe4SsP">pic.twitter.com/1oFkMe4SsP</a></p>&mdash; Zach #masks4all Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1269818072577331200?ref_src=twsrc%5Etfw">June 8, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://jianjye.com/p/deploy-fastai-digitalocean-ubuntu-flask-supervisor/">How to deploy Fastai on Ubuntu</a></p>
<h2 id="huggingface">HuggingFace</h2>
<p><a href="https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18">Deploying HuggingFace to Production</a></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/databuzzword/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/databuzzword/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/databuzzword/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is the DataBuzzWord Podcast Blog for AI/Data Addicts❤</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jqueguiner" title="jqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://stackoverflow.com/users/users%2F786021%2Fguignol" title="users/786021/guignol"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#stackoverflow"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/jlqueguiner" title="jlqueguiner"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jilijeanlouis" title="jilijeanlouis"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://www.youtube.com/UCwPhv5c2FQf6Ai3E6rT8_cw" title="UCwPhv5c2FQf6Ai3E6rT8_cw"><svg class="svg-icon grey"><use xlink:href="/databuzzword/assets/minima-social-icons.svg#youtube"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
